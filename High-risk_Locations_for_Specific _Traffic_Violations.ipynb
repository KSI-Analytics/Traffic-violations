{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "460463be",
   "metadata": {},
   "source": [
    "The Base data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a83699fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"Traffic_Violations.csv\")\n",
    "\n",
    "# Standardizeing column name\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdf1718e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting date and time\n",
    "df['date_of_stop'] = pd.to_datetime(df['date_of_stop'], errors='coerce')\n",
    "df['time_of_stop'] = pd.to_datetime(df['time_of_stop'], format='%H:%M:%S', errors='coerce').dt.time\n",
    "\n",
    "# Extracting the features for forecasting\n",
    "df['hour'] = pd.to_datetime(df['time_of_stop'], errors='coerce').dt.hour\n",
    "df['day_of_week'] = df['date_of_stop'].dt.day_name()\n",
    "df['month'] = df['date_of_stop'].dt.month\n",
    "\n",
    "# Converting coordinates to numeric for mapping\n",
    "df['latitude'] = pd.to_numeric(df['latitude'], errors='coerce')\n",
    "df['longitude'] = pd.to_numeric(df['longitude'], errors='coerce')\n",
    "\n",
    "# Convert the Yes/No or True/False columns to Boolean\n",
    "bool_cols = ['accident', 'belts', 'personal_injury', 'property_damage', 'fatal',\n",
    "             'commercial_license', 'hazmat', 'commercial_vehicle', 'alcohol', 'work_zone',\n",
    "             'search_conducted', 'search_person', 'search_vehicle', 'contraband_found',\n",
    "             'attributed_to_accident']\n",
    "for col in bool_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(str).str.lower().map({'yes': True, 'no': False, 'true': True, 'false': False})\n",
    "\n",
    "\n",
    "# Filling in missing categorical values\n",
    "fill_cols = ['gender', 'race', 'driver_city', 'driver_state', 'vehicle_type', 'make', 'model', 'arrest_type']\n",
    "for col in fill_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna('Unknown')\n",
    "\n",
    "# Dropping invalid or missing essential values\n",
    "df.dropna(subset=['date_of_stop', 'latitude', 'longitude'], inplace=True)\n",
    "df.drop_duplicates(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dcc3abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved as 'cleaned_traffic_violations.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.to_csv(\"cleaned_traffic_violations.csv\", index=False)\n",
    "print(\"Cleaned dataset saved as 'cleaned_traffic_violations.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabf331a-4031-4597-b893-9c02e11265fd",
   "metadata": {},
   "source": [
    "Cleaning the dataset to make it specific for this feature\n",
    "\n",
    "Reading the data from the csv and checking to make sure that I am accessing the dataset and identifying the current data types and removing any attributes I do not need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8102871e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   seqid date_of_stop time_of_stop agency  \\\n",
      "0   52282e8c-f2e1-4bb5-8509-2d5e4f8da8ca   2023-05-01     23:11:00    MCP   \n",
      "1   9be35886-e00c-49c2-8f27-2f6307696a17   2023-11-25     00:20:00    MCP   \n",
      "2   9be35886-e00c-49c2-8f27-2f6307696a17   2023-11-25     00:20:00    MCP   \n",
      "3   4d37fa99-0df3-4a56-9ba6-692bce894a34   2023-11-26     09:16:00    MCP   \n",
      "4   3a723e9a-5dc0-4bc3-9bd9-4555d6ce0e49   2023-11-25     05:45:00    MCP   \n",
      "5   3a723e9a-5dc0-4bc3-9bd9-4555d6ce0e49   2023-11-25     05:45:00    MCP   \n",
      "6   66273a8e-980e-413e-8928-56447e3be407   2023-11-25     01:21:00    MCP   \n",
      "7   78cdf309-9fe8-46de-892e-19e9b1bafacd   2023-11-25     03:22:00    MCP   \n",
      "8   78cdf309-9fe8-46de-892e-19e9b1bafacd   2023-11-25     03:22:00    MCP   \n",
      "9   1e24ec33-6a13-483f-9fea-0c2cdc5b2a14   2023-11-24     23:25:00    MCP   \n",
      "10  1e24ec33-6a13-483f-9fea-0c2cdc5b2a14   2023-11-24     23:25:00    MCP   \n",
      "11  1e24ec33-6a13-483f-9fea-0c2cdc5b2a14   2023-11-24     23:25:00    MCP   \n",
      "12  1e24ec33-6a13-483f-9fea-0c2cdc5b2a14   2023-11-24     23:25:00    MCP   \n",
      "13  1e24ec33-6a13-483f-9fea-0c2cdc5b2a14   2023-11-24     23:25:00    MCP   \n",
      "14  d2fca5aa-af7e-4294-86d9-78d43254dbd2   2023-11-25     00:47:00    MCP   \n",
      "\n",
      "                                          subagency  \\\n",
      "0                       3rd District, Silver Spring   \n",
      "1   6th District, Gaithersburg / Montgomery Village   \n",
      "2   6th District, Gaithersburg / Montgomery Village   \n",
      "3                             4th District, Wheaton   \n",
      "4                             4th District, Wheaton   \n",
      "5                             4th District, Wheaton   \n",
      "6                            2nd District, Bethesda   \n",
      "7                            2nd District, Bethesda   \n",
      "8                            2nd District, Bethesda   \n",
      "9                            2nd District, Bethesda   \n",
      "10                           2nd District, Bethesda   \n",
      "11                           2nd District, Bethesda   \n",
      "12                           2nd District, Bethesda   \n",
      "13                           2nd District, Bethesda   \n",
      "14                           2nd District, Bethesda   \n",
      "\n",
      "                                          description  \\\n",
      "0     OPERATING UNREGISTERED MOTOR VEHICLE ON HIGHWAY   \n",
      "1   FAILURE TO DISPLAY REGISTRATION CARD UPON DEMA...   \n",
      "2   DISPLAYING EXPIRED REGISTRATION PLATE ISSUED B...   \n",
      "3   DRIVING VEHICLE WHILE UNDER THE INFLUENCE OF A...   \n",
      "4   RECKLESS DRIVING VEHICLE IN WANTON AND WILLFUL...   \n",
      "5   DRIVER FAILURE TO OBEY PROPERLY PLACED TRAFFIC...   \n",
      "6   EXCEEDING POSTED MAXIMUM SPEED LIMIT: 44 MPH I...   \n",
      "7   DRIVER FAILURE TO STOP AT STEADY CIRCULAR RED ...   \n",
      "8   PERSON DRIVING MOTOR VEHICLE ON HIGHWAY OR PUB...   \n",
      "9   FAILURE OF INDIVIDUAL DRIVING ON HIGHWAY TO DI...   \n",
      "10  DRIVING, ATTEMPTING TO DRIVE MOTOR VEHICLE ON ...   \n",
      "11  FAILURE OF MV OPER. DRIVING ON HWY TO BE IN PO...   \n",
      "12  FAILURE OF MV OPER TO PRESENT EVIDENCE OF REQU...   \n",
      "13  OWNER FAILURE TO EXECUTE SECURITY INTEREST INF...   \n",
      "14  DRIVER FAILURE TO OBEY PROPERLY PLACED TRAFFIC...   \n",
      "\n",
      "                                 location   latitude  longitude  accident  \\\n",
      "0        BRIGGS CHANEY RD @ COLUMIBA PIKE   0.000000   0.000000     False   \n",
      "1                GEORGIA AVE / WEISMAN RD  39.052962 -77.051304     False   \n",
      "2                GEORGIA AVE / WEISMAN RD  39.052962 -77.051304     False   \n",
      "3                          3803 WELLER RD  39.058378 -77.049652     False   \n",
      "4   OLNEY LAYTONSVILLE RD @ FIELDCREST RD   0.000000   0.000000     False   \n",
      "5   OLNEY LAYTONSVILLE RD @ FIELDCREST RD   0.000000   0.000000     False   \n",
      "6      S/B GEORGIA AVENUE @ DENNIS AVENUE   0.000000   0.000000     False   \n",
      "7           16 TH STREET @ GEORGIA AVENUE   0.000000   0.000000     False   \n",
      "8           16 TH STREET @ GEORGIA AVENUE   0.000000   0.000000     False   \n",
      "9       UNIVERSITY BLVD W & VEIRS MILL RD  39.041932 -77.058799     False   \n",
      "10      UNIVERSITY BLVD W & VEIRS MILL RD  39.041932 -77.058799     False   \n",
      "11      UNIVERSITY BLVD W & VEIRS MILL RD  39.041932 -77.058799     False   \n",
      "12      UNIVERSITY BLVD W & VEIRS MILL RD  39.041932 -77.058799     False   \n",
      "13      UNIVERSITY BLVD W & VEIRS MILL RD  39.041932 -77.058799     False   \n",
      "14       RANDOLPH RD & ENGLISH ORCHARD CT  39.060892 -77.044710     False   \n",
      "\n",
      "    ...      race  gender    driver_city  driver_state  dl_state  \\\n",
      "0   ...     WHITE       M   GAITHERSBURG            MD        MD   \n",
      "1   ...  HISPANIC       M  SILVER SPRING            MD        MD   \n",
      "2   ...  HISPANIC       M  SILVER SPRING            MD        MD   \n",
      "3   ...  HISPANIC       M  SILVER SPRING            MD        MD   \n",
      "4   ...     WHITE       M   GAITHERSBURG            MD        MD   \n",
      "5   ...     WHITE       M   GAITHERSBURG            MD        MD   \n",
      "6   ...  HISPANIC       F  SILVER SPRING            MD        MD   \n",
      "7   ...     BLACK       M         JESSUP            MD        MD   \n",
      "8   ...     BLACK       M         JESSUP            MD        MD   \n",
      "9   ...  HISPANIC       M  SILVER SPRING            MD        MD   \n",
      "10  ...  HISPANIC       M  SILVER SPRING            MD        MD   \n",
      "11  ...  HISPANIC       M  SILVER SPRING            MD        MD   \n",
      "12  ...  HISPANIC       M  SILVER SPRING            MD        MD   \n",
      "13  ...  HISPANIC       M  SILVER SPRING            MD        MD   \n",
      "14  ...  HISPANIC       M         SEVERN            MD        MD   \n",
      "\n",
      "            arrest_type                            geolocation  hour  \\\n",
      "0     A - Marked Patrol                             (0.0, 0.0)   NaN   \n",
      "1     A - Marked Patrol        (39.0529625, -77.0513041666667)   NaN   \n",
      "2     A - Marked Patrol        (39.0529625, -77.0513041666667)   NaN   \n",
      "3     A - Marked Patrol  (39.0583783333333, -77.0496516666667)   NaN   \n",
      "4     A - Marked Patrol                             (0.0, 0.0)   NaN   \n",
      "5     A - Marked Patrol                             (0.0, 0.0)   NaN   \n",
      "6   B - Unmarked Patrol                             (0.0, 0.0)   NaN   \n",
      "7   B - Unmarked Patrol                             (0.0, 0.0)   NaN   \n",
      "8   B - Unmarked Patrol                             (0.0, 0.0)   NaN   \n",
      "9     A - Marked Patrol  (39.0419323833333, -77.0587989666667)   NaN   \n",
      "10    A - Marked Patrol  (39.0419323833333, -77.0587989666667)   NaN   \n",
      "11    A - Marked Patrol  (39.0419323833333, -77.0587989666667)   NaN   \n",
      "12    A - Marked Patrol  (39.0419323833333, -77.0587989666667)   NaN   \n",
      "13    A - Marked Patrol  (39.0419323833333, -77.0587989666667)   NaN   \n",
      "14    A - Marked Patrol  (39.0608920833333, -77.0447097333333)   NaN   \n",
      "\n",
      "    day_of_week month  \n",
      "0        Monday     5  \n",
      "1      Saturday    11  \n",
      "2      Saturday    11  \n",
      "3        Sunday    11  \n",
      "4      Saturday    11  \n",
      "5      Saturday    11  \n",
      "6      Saturday    11  \n",
      "7      Saturday    11  \n",
      "8      Saturday    11  \n",
      "9        Friday    11  \n",
      "10       Friday    11  \n",
      "11       Friday    11  \n",
      "12       Friday    11  \n",
      "13       Friday    11  \n",
      "14     Saturday    11  \n",
      "\n",
      "[15 rows x 46 columns]\n",
      "\n",
      "\n",
      "Exploring the Data\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1994277 entries, 0 to 1994276\n",
      "Data columns (total 46 columns):\n",
      " #   Column                   Dtype  \n",
      "---  ------                   -----  \n",
      " 0   seqid                    object \n",
      " 1   date_of_stop             object \n",
      " 2   time_of_stop             object \n",
      " 3   agency                   object \n",
      " 4   subagency                object \n",
      " 5   description              object \n",
      " 6   location                 object \n",
      " 7   latitude                 float64\n",
      " 8   longitude                float64\n",
      " 9   accident                 bool   \n",
      " 10  belts                    bool   \n",
      " 11  personal_injury          bool   \n",
      " 12  property_damage          bool   \n",
      " 13  fatal                    bool   \n",
      " 14  commercial_license       bool   \n",
      " 15  hazmat                   bool   \n",
      " 16  commercial_vehicle       bool   \n",
      " 17  alcohol                  bool   \n",
      " 18  work_zone                bool   \n",
      " 19  search_conducted         object \n",
      " 20  search_disposition       object \n",
      " 21  search_outcome           object \n",
      " 22  search_reason            object \n",
      " 23  search_reason_for_stop   object \n",
      " 24  search_type              object \n",
      " 25  search_arrest_reason     object \n",
      " 26  state                    object \n",
      " 27  vehicletype              object \n",
      " 28  year                     float64\n",
      " 29  make                     object \n",
      " 30  model                    object \n",
      " 31  color                    object \n",
      " 32  violation_type           object \n",
      " 33  charge                   object \n",
      " 34  article                  object \n",
      " 35  contributed_to_accident  bool   \n",
      " 36  race                     object \n",
      " 37  gender                   object \n",
      " 38  driver_city              object \n",
      " 39  driver_state             object \n",
      " 40  dl_state                 object \n",
      " 41  arrest_type              object \n",
      " 42  geolocation              object \n",
      " 43  hour                     float64\n",
      " 44  day_of_week              object \n",
      " 45  month                    int64  \n",
      "dtypes: bool(11), float64(4), int64(1), object(30)\n",
      "memory usage: 553.5+ MB\n",
      "None\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#I only want to do this section once\n",
    "\n",
    "#Reading from the base cleaned csv & loading the dataframe\n",
    "data = pd.read_csv(\"cleaned_traffic_violations.csv\")\n",
    "\n",
    "#Testing if it read from the csv\n",
    "print(data.head(15)) \n",
    "print('\\n')\n",
    "\n",
    "#Before \n",
    "\n",
    "#Identifying the current data types\n",
    "print(\"Exploring the Data\")\n",
    "print(data.info())\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04068a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for this feature I only require these attributes [latitude, longitude, accident, belts, personal_injury, property_damage, fatal, hazmat, alcohol]\n",
    "dropping_these_attributes = [\"seqid\", \"date_of_stop\", \"time_of_stop\", \"agency\", \"subagency\", \"description\", \"location\", \"commercial_license\", \"commercial_vehicle\", \"work_zone\", \"search_conducted\", \"search_disposition\", \"search_outcome\", \"search_reason\", \"search_reason_for_stop\", \"search_type\", \"search_arrest_reason\", \"state\", \"vehicletype\", \"year\", \"make\", \"model\", \"color\", \"violation_type\", \"charge\", \"article\", \"contributed_to_accident\", \"race\", \"gender\", \"driver_city\", \"driver_state\", \"dl_state\", \"arrest_type\", \"geolocation\", \"hour\", \"day_of_week\", \"month\"]\n",
    "data.drop(columns=dropping_these_attributes, inplace= True, errors= \"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e00cca",
   "metadata": {},
   "source": [
    "For the attributes I did keep, for the categorical attributes I want to check for uniqueness of categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9a1a16f-95a0-4d3d-b164-2fb405762946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Accident Values\n",
      "[0 1]\n",
      "\n",
      "\n",
      "Unique Belts Values\n",
      "[0 1]\n",
      "\n",
      "\n",
      "Unique Personal Injury Values\n",
      "[0 1]\n",
      "\n",
      "\n",
      "Unique Property Damage Values\n",
      "[0 1]\n",
      "\n",
      "\n",
      "Unique Fatal Values\n",
      "[0 1]\n",
      "\n",
      "\n",
      "Unique HAZMAT Values\n",
      "[0 1]\n",
      "\n",
      "\n",
      "Unique Alcohol Values\n",
      "[0 1]\n",
      "\n",
      "\n",
      "========== Missing Value Counts ==========\n",
      "latitude           0\n",
      "longitude          0\n",
      "accident           0\n",
      "belts              0\n",
      "personal_injury    0\n",
      "property_damage    0\n",
      "fatal              0\n",
      "hazmat             0\n",
      "alcohol            0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Exploring the Data\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1994277 entries, 0 to 1994276\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Dtype  \n",
      "---  ------           -----  \n",
      " 0   latitude         float32\n",
      " 1   longitude        float32\n",
      " 2   accident         int32  \n",
      " 3   belts            int32  \n",
      " 4   personal_injury  int32  \n",
      " 5   property_damage  int32  \n",
      " 6   fatal            int32  \n",
      " 7   hazmat           int32  \n",
      " 8   alcohol          int32  \n",
      "dtypes: float32(2), int32(7)\n",
      "memory usage: 68.5 MB\n",
      "None\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#I want to test this section multiple times\n",
    "\n",
    "#Checking the values in my categorical attributes to ensure correct groupings\n",
    "\n",
    "print(\"Unique Accident Values\")\n",
    "print(data[\"accident\"].unique())\n",
    "print('\\n')\n",
    "\n",
    "print(\"Unique Belts Values\")\n",
    "print(data[\"belts\"].unique())\n",
    "print('\\n')\n",
    "\n",
    "print(\"Unique Personal Injury Values\")\n",
    "print(data[\"personal_injury\"].unique())\n",
    "print('\\n')\n",
    "\t\n",
    "print(\"Unique Property Damage Values\")\n",
    "print(data[\"property_damage\"].unique())\n",
    "print('\\n')\n",
    "\n",
    "print(\"Unique Fatal Values\")\n",
    "print(data[\"fatal\"].unique())\n",
    "print('\\n')\n",
    "\n",
    "print(\"Unique HAZMAT Values\")\n",
    "print(data[\"hazmat\"].unique())\n",
    "print('\\n')\n",
    "\n",
    "print(\"Unique Alcohol Values\")\n",
    "print(data[\"alcohol\"].unique())\n",
    "print('\\n')\n",
    "\n",
    "#Checking for missing fields\n",
    "print('========== Missing Value Counts ==========')\n",
    "print(data.isnull().sum())\n",
    "print('\\n')\n",
    "\n",
    "#converting the yes's and no's to 1's and 0's\n",
    "list_of_yes_no_columns = [\"accident\", \"belts\", \"personal_injury\", \"property_damage\", \"fatal\",\"hazmat\", \"alcohol\"]\n",
    "data[list_of_yes_no_columns] = data[list_of_yes_no_columns].replace({True: 1, False: 0})\n",
    "\n",
    "#casting datatypes and downcasting for reduced memory usage\n",
    "data[list_of_yes_no_columns] = data[list_of_yes_no_columns].astype(\"int32\")\n",
    "\n",
    "lat_long = [\"latitude\", \"longitude\"]\n",
    "data[lat_long] = data[lat_long].astype(\"float32\")\n",
    "\n",
    "#After \n",
    "#Testing for changes\n",
    "print(\"Exploring the Data\")\n",
    "print(data.info())\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58074397",
   "metadata": {},
   "source": [
    "For the attributes I am using for this feature, there are no missing or duplicate category values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4390399",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I only want to do this section once\n",
    "\n",
    "#updating the csv with the changes \n",
    "data.to_csv(\"cleaned_traffic_violations.csv\", index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6cea09",
   "metadata": {},
   "source": [
    "Model 1: Logic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5c2df74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ee4fec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your location details: Latitude and Longitude \n",
      "\n",
      "Top Most Likely Traffic Violations Based on Location\n",
      "\n",
      "\n",
      "belts: 0.03\n",
      "accident: 0.03\n",
      "property_damage: 0.02\n",
      "personal_injury: 0.01\n"
     ]
    }
   ],
   "source": [
    "features = [\"latitude\", \"longitude\"]\n",
    "violations = [\"accident\", \"belts\", \"personal_injury\", \"property_damage\", \"fatal\", \"hazmat\",\"alcohol\"]\n",
    "\n",
    "X = data[features].values\n",
    "y = data[violations].values\n",
    "\n",
    "#Training the model\n",
    "models={}\n",
    "for loc, violation in enumerate(violations):\n",
    "  logr = LogisticRegression(max_iter=1000)\n",
    "  logr.fit(X, y [:, loc])\n",
    "  models[violation] = logr\n",
    "\n",
    "def probability_function (logr, user_input_values):\n",
    "  return logr.predict_proba([user_input_values])[0][1]\n",
    "   \n",
    "\n",
    "#User Inputs\n",
    "print(\"Enter your location details: Latitude and Longitude \\n\")\n",
    "\n",
    "latitude_input_value = float(input(\"Enter your Latitude: \"))\n",
    "longitude_input_value = float(input(\"Enter your Longitude: \"))\n",
    "\n",
    "user_input_values = [latitude_input_value, longitude_input_value]\n",
    "\n",
    "result = []\n",
    "for v in violations:\n",
    "  p = probability_function(models[v], user_input_values)\n",
    "  result.append(p)\n",
    "\n",
    "top_viol = sorted(zip(violations, result), key=lambda p: p[1], reverse=True)[:4]\n",
    "\n",
    "print(\"Top Most Likely Traffic Violations Based on Location\")\n",
    "print(\"\\n\")\n",
    "\n",
    "for vio, prob in top_viol:\n",
    "  print(f\"{vio}: {prob:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead216e4",
   "metadata": {},
   "source": [
    "Model 2: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cc05f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3cd016",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['Latitude', 'Longitude'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLatitude\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLongitude\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      2\u001b[0m violations \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccident\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelts\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPersonal Injury\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProperty Damage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFatal\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHAZMAT\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlcohol\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 4\u001b[0m X \u001b[38;5;241m=\u001b[39m data[features]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m      5\u001b[0m y \u001b[38;5;241m=\u001b[39m data[violations]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#Training the model\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['Latitude', 'Longitude'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "features = [\"latitude\", \"longitude\"]\n",
    "violations = [\"accident\", \"belts\", \"personal_injury\", \"property_damage\", \"fatal\", \"hazmat\",\"alcohol\"]\n",
    "\n",
    "X = data[features].values\n",
    "y = data[violations].values\n",
    "\n",
    "#Training the model\n",
    "models={}\n",
    "for loc, violation in enumerate(violations):\n",
    "  rfc = RandomForestClassifier(n_estimators= 10, random_state=5)\n",
    "  rfc.fit(X, y [:, loc])\n",
    "  models[violation] = rfc\n",
    "\n",
    "def probability_function (rfc, user_input_values):\n",
    "  return rfc.predict_proba([user_input_values])[0][1]\n",
    "   \n",
    "\n",
    "#User Inputs\n",
    "print(\"Enter your location details: Latitude and Longitude \\n\")\n",
    "\n",
    "latitude_input_value = float(input(\"Enter your Latitude: \"))\n",
    "longitude_input_value = float(input(\"Enter your Longitude: \"))\n",
    "\n",
    "user_input_values = [latitude_input_value, longitude_input_value]\n",
    "\n",
    "result = []\n",
    "for v in violations:\n",
    "  p = probability_function(models[v], user_input_values)\n",
    "  result.append(p)\n",
    "\n",
    "top_viol = sorted(zip(violations, result), key=lambda p: p[1], reverse=True)[:4]\n",
    "\n",
    "print(\"Top Most Likely Traffic Violations Based on Location\")\n",
    "print(\"\\n\")\n",
    "\n",
    "for vio, prob in top_viol:\n",
    "  print(f\"{vio}: {prob:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb31cdf0",
   "metadata": {},
   "source": [
    "Model 3: Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ef1e1ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "18e010ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your location details: Latitude and Longitude \n",
      "\n",
      "Top Most Likely Traffic Violations Based on Location\n",
      "\n",
      "\n",
      "Accident: 0.03\n",
      "Belts: 0.03\n",
      "Property Damage: 0.02\n",
      "Personal Injury: 0.01\n"
     ]
    }
   ],
   "source": [
    "features = [\"Latitude\", \"Longitude\"]\n",
    "violations = [\"Accident\", \"Belts\", \"Personal Injury\", \"Property Damage\", \"Fatal\", \"HAZMAT\",\"Alcohol\"]\n",
    "\n",
    "X = data[features].values\n",
    "y = data[violations].values\n",
    "\n",
    "#Training the model\n",
    "models={}\n",
    "for loc, violation in enumerate(violations):\n",
    "  gbc = GradientBoostingClassifier(n_estimators= 10, learning_rate=0.2)\n",
    "  gbc.fit(X, y [:, loc])\n",
    "  models[violation] = gbc\n",
    "\n",
    "def probability_function (gbc, user_input_values):\n",
    "  return gbc.predict_proba([user_input_values])[0][1]\n",
    "   \n",
    "\n",
    "#User Inputs\n",
    "print(\"Enter your location details: Latitude and Longitude \\n\")\n",
    "\n",
    "latitude_input_value = float(input(\"Enter your Latitude: \"))\n",
    "longitude_input_value = float(input(\"Enter your Longitude: \"))\n",
    "\n",
    "user_input_values = [latitude_input_value, longitude_input_value]\n",
    "\n",
    "result = []\n",
    "for v in violations:\n",
    "  p = probability_function(models[v], user_input_values)\n",
    "  result.append(p)\n",
    "\n",
    "top_viol = sorted(zip(violations, result), key=lambda p: p[1], reverse=True)[:4]\n",
    "\n",
    "print(\"Top Most Likely Traffic Violations Based on Location\")\n",
    "print(\"\\n\")\n",
    "\n",
    "for vio, prob in top_viol:\n",
    "  print(f\"{vio}: {prob:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efe18fc",
   "metadata": {},
   "source": [
    "Models Accuracy, Mean Absoute Error and Root Mean Squre Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fa622fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accident\n",
      "Random Forest Classifier\n",
      "Accuracy:  0.98\n",
      "MAE: 0.02\n",
      "RSME:  0.13\n",
      "\n",
      "\n",
      "Gradient Boosting Classifier\n",
      "Accuracy:  0.97\n",
      "MAE: 0.03\n",
      "RSME:  0.17\n",
      "\n",
      "\n",
      "Logistic Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\katoy\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.97\n",
      "MAE: 0.03\n",
      "RSME:  0.17\n",
      "\n",
      "\n",
      "Belts\n",
      "Random Forest Classifier\n",
      "Accuracy:  0.98\n",
      "MAE: 0.02\n",
      "RSME:  0.13\n",
      "\n",
      "\n",
      "Gradient Boosting Classifier\n",
      "Accuracy:  0.97\n",
      "MAE: 0.03\n",
      "RSME:  0.18\n",
      "\n",
      "\n",
      "Logistic Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\katoy\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.97\n",
      "MAE: 0.03\n",
      "RSME:  0.18\n",
      "\n",
      "\n",
      "Personal Injury\n",
      "Random Forest Classifier\n",
      "Accuracy:  0.99\n",
      "MAE: 0.01\n",
      "RSME:  0.07\n",
      "\n",
      "\n",
      "Gradient Boosting Classifier\n",
      "Accuracy:  0.99\n",
      "MAE: 0.01\n",
      "RSME:  0.11\n",
      "\n",
      "\n",
      "Logistic Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\katoy\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.99\n",
      "MAE: 0.01\n",
      "RSME:  0.11\n",
      "\n",
      "\n",
      "Property Damage\n",
      "Random Forest Classifier\n",
      "Accuracy:  0.99\n",
      "MAE: 0.01\n",
      "RSME:  0.10\n",
      "\n",
      "\n",
      "Gradient Boosting Classifier\n",
      "Accuracy:  0.98\n",
      "MAE: 0.02\n",
      "RSME:  0.15\n",
      "\n",
      "\n",
      "Logistic Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\katoy\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.98\n",
      "MAE: 0.02\n",
      "RSME:  0.15\n",
      "\n",
      "\n",
      "Fatal\n",
      "Random Forest Classifier\n",
      "Accuracy:  1.00\n",
      "MAE: 0.00\n",
      "RSME:  0.01\n",
      "\n",
      "\n",
      "Gradient Boosting Classifier\n",
      "Accuracy:  1.00\n",
      "MAE: 0.00\n",
      "RSME:  0.02\n",
      "\n",
      "\n",
      "Logistic Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\katoy\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.00\n",
      "MAE: 0.00\n",
      "RSME:  0.02\n",
      "\n",
      "\n",
      "HAZMAT\n",
      "Random Forest Classifier\n",
      "Accuracy:  1.00\n",
      "MAE: 0.00\n",
      "RSME:  0.01\n",
      "\n",
      "\n",
      "Gradient Boosting Classifier\n",
      "Accuracy:  1.00\n",
      "MAE: 0.00\n",
      "RSME:  0.01\n",
      "\n",
      "\n",
      "Logistic Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\katoy\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.00\n",
      "MAE: 0.00\n",
      "RSME:  0.01\n",
      "\n",
      "\n",
      "Alcohol\n",
      "Random Forest Classifier\n",
      "Accuracy:  1.00\n",
      "MAE: 0.00\n",
      "RSME:  0.02\n",
      "\n",
      "\n",
      "Gradient Boosting Classifier\n",
      "Accuracy:  1.00\n",
      "MAE: 0.00\n",
      "RSME:  0.04\n",
      "\n",
      "\n",
      "Logistic Regression\n",
      "Accuracy:  1.00\n",
      "MAE: 0.00\n",
      "RSME:  0.04\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\katoy\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "#training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size= 0.3, random_state=5)\n",
    "\n",
    "#Setting the models\n",
    "\n",
    "m1= RandomForestClassifier(n_estimators=10, random_state=5)\n",
    "m2= GradientBoostingClassifier(n_estimators=10, learning_rate= 0.2)\n",
    "m3= LogisticRegression(max_iter=10)\n",
    "\n",
    "models_set = {\n",
    "    \"Random Forest Classifier\":m1,\n",
    "    \"Gradient Boosting Classifier\":m2,\n",
    "    \"Logistic Regression\":m3\n",
    "}\n",
    "\n",
    "for v, violation in enumerate(violations):\n",
    "    print(f\"{violation}\")\n",
    "    \n",
    "    for name, model in models_set.items():\n",
    "        print(f\"{name}\")\n",
    "\n",
    "        model.fit(X_train, y_train[:, v])\n",
    "\n",
    "        m_pred = model.predict(X_test)\n",
    "\n",
    "        model_accuracy = accuracy_score(y_test[:, v], m_pred)\n",
    "        mae = mean_absolute_error(y_test[:, v], m_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test[:, v], m_pred))\n",
    "\n",
    "        print(f\"Accuracy: {model_accuracy: .2f}\")\n",
    "        print(f\"MAE:{mae: .2f}\")\n",
    "        print(f\"RSME: {rmse: .2f}\")\n",
    "        print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14827f91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
